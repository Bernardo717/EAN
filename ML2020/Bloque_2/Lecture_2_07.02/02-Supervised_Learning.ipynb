{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/EAN.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/0_intro_ml.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Supervised Learning\n",
    "\n",
    "## Instructors:\n",
    "\n",
    ">Leonardo A. Espinosa, PhD. Instructor.\n",
    "(*email*: leonardo.espinosaleal@arcada.fi)\n",
    "\n",
    "> Ruben D. Acosta, MSc. Instructor.\n",
    "(*email*:  rdacostav@universidadean.edu.co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal for today\n",
    "* Understand the principles of supervised learning.\n",
    "* Identify the pros and cons of the main algorithms for regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ai_ml_dl.png\" style=\"width:1400px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Statistical Learning\n",
    "\n",
    "**framework for machine learning drawing from the fields of *statistics* and *functional analysis*.**\n",
    "\n",
    "* It deals with the problem of finding a predictive function (**f**) based on data.\n",
    "\n",
    "* It is basically a set of tools for *understanding the data*.\n",
    "\n",
    ">**Machine learning** is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) from data, without being explicitly programmed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Machine learning\n",
    "\n",
    ">* Supervised learning\n",
    ">* Unsupervised learning\n",
    ">* Reinforcement learning\n",
    "\n",
    "#### Others:\n",
    ">* Semi-supervised learning\n",
    ">* Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning (*Supervised Statistical Learning*)\n",
    "\n",
    ">Building a statistical model for predicting, or estimating, an *output* based on one or more *inputs*. \n",
    "\n",
    ">Range of disciplines: Bussines, medicine, astrophysics, public policy , social sciences and many more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification and Regression\n",
    "\n",
    "* >Classification $\\to$ qualitative or categorical variables.\n",
    "* >Regression $\\to$ Continuos numerical quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization, Overfitting and Underfitting\n",
    "\n",
    "#### Generalization\n",
    "Build a model on the training set and then be able to make predictions on **\"new data\"**.\n",
    "\n",
    "#### Overfitting\n",
    "Building a model that is too complex for the amount of available information.\n",
    "\n",
    "#### Underfitting\n",
    "Choosing a too simple model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "> **Bias** ― The bias of a model is the difference between the expected prediction and the correct model that we try to predict for given data points.\n",
    "\n",
    "> **Variance** ― The variance of a model is the variability of the model prediction for given data points.\n",
    "\n",
    "> **Bias/variance tradeoff** ― The simpler the model, the higher the bias, and the more complex the model, the higher the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/bias-variance.png\" style=\"width:1200px\">\n",
    "Figure 1b. Example of Fitting-Underfitting models in classification and regression.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Possible Remedies\n",
    "\n",
    ">Underfitting:\n",
    "* Complexity model\n",
    "* Add more features\n",
    "\n",
    ">Overfitting:\n",
    "* Perform regularization\n",
    "* Get more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/02-overfitting_underfitting.png\" style=\"width:1000px\">\n",
    "Figure 1c: Trade-off of model complexity againts *training* and *test* accuracy.\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relation of Model Complexity to Dataset Size\n",
    "Having more data and building appropiately more complex models.\n",
    "However data alone is not enough.\n",
    "\n",
    "Always remember:\n",
    "\n",
    "1. Is generalization that counts.\n",
    "\n",
    "2. The *curse of dimensionality* and the *blessing of non-uniformity*.\n",
    "\n",
    "3. No free-lunch theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/cod.jpg\" style=\"width:1000px\">\n",
    "Figure 1d: The curse of dimensionality and the blessing of non-uniformity.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/nflt.jpg\" style=\"width:1000px\">\n",
    "Figure 1e: No free-lunch theorem\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Machine Learning Algorithms\n",
    "\n",
    "In this Lecture we are going to explore, using examples, the main algorithms for supervised learning, following a taxonomic approach, including pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1\\. <a href=\"#/32/1\">k-nearest neighbors (KNN)</a>:\n",
    "   * for classification.\n",
    "   * for regression.\n",
    "   \n",
    "2\\. <a href=\"#/50/1\">Linear Models</a>:\n",
    "   * <a href=\"#/51/1\">for Regression</a>:\n",
    "       * Linear regression *aka* least squares.\n",
    "       * Ridge.\n",
    "       * Lasso.\n",
    "       * Elastic Net.\n",
    "   * <a href=\"#/65/1\">for Classification</a>:\n",
    "       * Logistic regression.\n",
    "       * Linear Suppor Vector Machines.\n",
    "       * Linear models for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3\\. <a href=\"#/83\">Naive-Bayes Classifiers</a>\n",
    "   \n",
    "4\\. <a href=\"#/89/1\">Decision Trees</a>:\n",
    "   * for classification.\n",
    "   * for regression.\n",
    "\n",
    "5\\. <a href=\"#/99/1\">Ensembles of Decision Trees</a>:\n",
    "   * Random Forest.\n",
    "   * Gradient Boosted Decision Trees.\n",
    "       \n",
    "6\\. <a href=\"#/109/1\">Kernelized Support Vector Machines</a>\n",
    "\n",
    "7\\. <a href=\"#/130/1\">Conclusions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Abalone dataset\n",
    "\n",
    "<center>    \n",
    "<img src=\"./images/02-abalon.jpg\" style=\"width:1000px\">\n",
    "Figure 2: Abalones' picture.    \n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The main question:\n",
    "Predict the age of abalone from physical measurements\n",
    "\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Attribute Information:\n",
    "\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem. \n",
    "more info [https://archive.ics.uci.edu/ml/datasets/Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Name |            Data Type|       Meas.|   Description\n",
    "-|-|-|-\n",
    "Sex           |  nominal    |           |  M, F, and I (infant)\n",
    "Length        |  continuous |     mm    |   Longest shell measurement\n",
    "Diameter      |  continuous |     mm    |  perpendicular to length\n",
    "Height        |  continuous |     mm    |  with meat in shell\n",
    "Whole weight  |  continuous |     grams |  whole abalone\n",
    "Shucked weight|  continuous |     grams |  weight of meat\n",
    "Viscera weight|  continuous |     grams |  gut weight (after bleeding)\n",
    "Shell weight  |  continuous |     grams |  after being dried\n",
    "Rings         |  integer    |           |  +1.5 gives the age in years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/tabla.png\" style=\"width:1200px\">\n",
    "Table 1: List of the abalone's dataset features.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/scikit-learn-logo.png\" style=\"width:1000px\">\n",
    "**http://scikit-learn.org**\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">Users: Spotify, Evernote, Booking.com a, OKCupid and many others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_map.png\" style=\"width:1100px\">\n",
    "http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/ml_map-cr.png\" style=\"width:1600px\">\n",
    "    Today's Lecture\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mglearn\n",
    "\n",
    "from matplotlib import rc\n",
    "font = {'family' : 'monospace', 'weight' : 'bold', 'size'   : 25}\n",
    "rc('font', **font) \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['lines.linewidth'] = 5.0\n",
    "plt.rcParams['lines.markersize'] = 15.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "file_path='https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\n",
    "names = ['Sex','Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings']\n",
    "df = pd.read_csv(file_path,header=None,names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We add a Years column  \n",
    "df['Years'] = df['Rings'] + 1.5\n",
    "\n",
    "# We change the M,F and I categorical variables as numerical using 0,1 and 2.\n",
    "replace_list = {\"Sex\" : {\"M\": 0, \"F\" : 1, \"I\": 2}}\n",
    "df.replace(replace_list,inplace=True)\n",
    "# If we want, we can inspect the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here we turn into numpy arrays\n",
    "X = df.iloc[:,:8].values\n",
    "y_cls = df.iloc[:,8].values\n",
    "y_reg = df.iloc[:,9].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# *k*-Nearest Neighbors \n",
    "\n",
    "The most intuitive algorithm. There are two versions:\n",
    "\n",
    "1. ### *k*-Neighbors for classification \n",
    "2. ### *k*-Neighbors for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/02-knns.png\" alt=\"Drawing\" style=\"width: 1200px;\"/>\n",
    "<strong>Figure 3:</strong> KNN examples. <strong>Top</strong>: K=1 and <strong>bottom</strong>: K=3. <strong>Left</strong>: for classification and <strong>right</strong>: for regression.        \n",
    "</center>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cls, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Abalon Dataset\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to any number\n",
    "neighbors_settings = list(range(1, 50))\n",
    "for n_neighbors in neighbors_settings:\n",
    "# build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "# record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "# record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylim([0.2,0.35])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing KNeighborsClassifier: Benchmark example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def knn_test():\n",
    "    Xt, yt = mglearn.datasets.make_forge()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # the fit method returns the object self, so we can instantiate\n",
    "    # and fit in one line\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(Xt, yt)\n",
    "        mglearn.plots.plot_2d_separator(clf, Xt, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    "        mglearn.discrete_scatter(Xt[:, 0], Xt[:, 1], yt, ax=ax)\n",
    "        ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    "        ax.set_xlabel(\"feature 0\")\n",
    "        ax.set_ylabel(\"feature 1\")\n",
    "        axes[0].legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=0)\n",
    "reg = KNeighborsRegressor(n_neighbors=10)\n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Abalon dataset\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to wherever\n",
    "neighbors_settings = list(range(1, 100))\n",
    "for n_neighbors in neighbors_settings:\n",
    "# build the model\n",
    "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    reg.fit(X_train, y_train)\n",
    "# record training set accuracy\n",
    "    training_accuracy.append(reg.score(X_train, y_train))\n",
    "# record generalization accuracy\n",
    "    test_accuracy.append(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_abalon_knn():\n",
    "    plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "    plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "    plt.ylabel(\"Accuracy ($R^2$)\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_abalon_knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_abalon_knn_zoom():\n",
    "    plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "    plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "    plt.ylabel(\"Accuracy ($R^2$)\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.ylim([0.4,0.65])\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_abalon_knn_zoom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analyzing KNeighborsRegressor: Benchmark example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def knn_regressor():\n",
    "    X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "    line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "    for n_neighbors, ax in zip([1, 3, 20], axes):\n",
    "    # make predictions using 1, 3, or 9 neighbors\n",
    "        reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        reg.fit(X_train2, y_train2)\n",
    "        ax.plot(line, reg.predict(line))\n",
    "        ax.plot(X_train2, y_train2, '^', c=mglearn.cm2(0), markersize=8)\n",
    "        ax.plot(X_test2, y_test2, 'v', c=mglearn.cm2(1), markersize=8)\n",
    "\n",
    "        ax.set_title(\"{} neighbor(s)\\n train score: {:.2f}\\n test score: {:.2f}\".format(n_neighbors, \n",
    "                    reg.score(X_train2, y_train2),reg.score(X_test2, y_test2)))\n",
    "        ax.set_xlabel(\"Feature\")\n",
    "        ax.set_ylabel(\"Target\")\n",
    "\n",
    "    axes[0].legend([\"Model predictions\", \"Training data/target\",\"Test data/target\"], loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_regressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions on the *KNN* algorithms\n",
    "\n",
    "* Two important parameters: the number of neighbors and how you measure the distance between points. By default is the Minkowski with p=2.\n",
    "\n",
    "$$ d(\\mathbf{x},\\mathbf{y}) = \\left[\\sum_{i=1}^N (x_i - y_i)^p \\right]^{\\frac{1}{p}} $$\n",
    "\n",
    "* It is a model easy to understand. But its perform is poor on large datasets (either in number of features or in number of samples) or sparse data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Fit a K-NN model for the wine dataset for predicting:\n",
    "\n",
    "  1. The level of alcohol, and\n",
    "  2. The type of wine.\n",
    "  \n",
    "Test different values of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Models\n",
    "\n",
    "Linear models make a prediction using a linear function of the input features.\n",
    "\n",
    "\n",
    "## Linear models for regression and linear models for classification.\n",
    "\n",
    "$$\\hat{y}(\\mathbf{w},\\mathbf{x}) = w_0 +  w_1 * x_1 + w_2 * x_2 + ... + w_p * x_p $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models for Regression\n",
    "\n",
    "   * Ordinary Least squares\n",
    "   $$ \\underset{w}{min\\,} {|| X w - y||_2}^2  $$\n",
    "      \n",
    "   * Ridge (L2 regularization)\n",
    "   $$  \\underset{w}{min\\,} {{|| X w - y||_2}^2 + \\alpha {||w||_2}^2} $$\n",
    "   \n",
    "   * Lasso (L1 regularization)\n",
    "   $$ \\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1} $$\n",
    "   \n",
    "   \n",
    "If $X$ is a matrix of size $(n, p)$ this methods have a cost of $O(n p^2)$, assuming that $n \\geq p$.\n",
    "\n",
    "   * Elastic Net (L2 and L1 regularization)\n",
    "   $$ \\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha \\rho ||w||_1 + \\frac{\\alpha(1-\\rho)}{2} ||w||_2 ^ 2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/lin-reg.png\" alt=\"Drawing\" style=\"width: 1400px;\"/>\n",
    "Figure 3: Regularization methods.        \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=42)\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge01 = Ridge(alpha=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ridge():\n",
    "    plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "    plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "    plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.01\")\n",
    "    plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "    plt.xticks(range(len(names)-1), names, rotation=90)\n",
    "    plt.hlines(0, 0, range(len(names)-1))\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.hlines(0, 0, len(lr.coef_))\n",
    "    plt.ylim(-25, 25)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import learning_curve, KFold\n",
    "\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(\n",
    "        est, X, y, train_sizes=np.linspace(.1, 1, 20), cv=KFold(20, shuffle=True, random_state=1))\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--',\n",
    "                    label=\"training \" + estimator_name)\n",
    "    plt.plot(training_set_size, test_scores.mean(axis=1), '-',\n",
    "             label=\"test \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Score (R^2)')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "\n",
    "def plot_ridge_n_samples(X,y,alpha=1):\n",
    "    plot_learning_curve(Ridge(alpha=alpha), X, y)\n",
    "    plot_learning_curve(LinearRegression(), X, y)\n",
    "    plt.legend(loc=(0, 1.05), ncol=2, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "plot_ridge_n_samples(X,y_reg,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_b, y_b = mglearn.datasets.load_extended_boston()\n",
    "plot_ridge_n_samples(X_b,y_b,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, random_state=42)\n",
    "\n",
    "lasso = Lasso(max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_lasso_vs_ridge():\n",
    "    plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "    plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "    plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "    plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "    plt.xticks(range(len(names)-1), names, rotation=90)\n",
    "    plt.hlines(0, 0, range(len(names)-1))\n",
    "    plt.legend(ncol=2, loc=(0, 1.05))\n",
    "    plt.ylim(-25, 25)\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lasso_vs_ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions on  Linear Models for Regression\n",
    "* Despite their simplicity, linear models for regression are widely used in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models for Classification\n",
    "\n",
    "* Logistic Regression (with L1 or L2 regularization)\n",
    "$$\\underset{w, c}{min\\,} \\|w\\|_1 \\quad or\\quad \\underset{w, c}{min\\,} \\frac{1}{2}w^T w  \\quad + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .\n",
    "$$\n",
    "\n",
    "* Linear Support Vector Machines  (for $x_i \\in \\mathbb{R}^p, i=1,…, n,$ and $y \\in \\{1, -1\\}^n$)\n",
    "$$ \\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i \\quad \\textrm {subject to }\\quad  y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\  \\zeta_i \\geq 0, i=1, ..., n $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/lr-svm.jpg\" alt=\"Drawing\" style=\"width: 1750px;\"/>\n",
    "Figure 4. Logistic Regression (*left*) and LSVM (*right*).\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reformulate the problem using the Abalone dataset, now binary Male or Female is the target.\n",
    "#First remove the rows for Sex I (Infant) = 2.\n",
    "\n",
    "df_bin = df[df.Sex !=2]\n",
    "\n",
    "# Here we turn into numpy arrays\n",
    "X_bin = df_bin.iloc[:,1:].values\n",
    "y_bin = df_bin.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, random_state=42)\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "new_names = ['Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings','Years']\n",
    "\n",
    "# Logistic Regression with L2 regularization\n",
    "\n",
    "def plot_lg_l2():\n",
    "    plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "    plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "    plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "    plt.xticks(range(len(new_names)), new_names, rotation=90)\n",
    "    plt.hlines(0, 0, range(len(new_names)))\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lg_l2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression with L1 regularization\n",
    "\n",
    "def plot_lg_l1():\n",
    "    for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "        lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
    "        #print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "            #C, lr_l1.score(X_train, y_train)))\n",
    "        #print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "            #C, lr_l1.score(X_test, y_test)))\n",
    "        plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "    plt.xticks(range(len(new_names)), new_names, rotation=90)\n",
    "    plt.hlines(0, 0, range(len(new_names)))\n",
    "    plt.xlabel(\"Coefficient index\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lg_l1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear models for multiclass classification\n",
    "\n",
    "* one-vs.-rest approach\n",
    "\n",
    ">caution: one-vs.-rest classifiers -linear ones- read the target labels in scalar mode [0,1,2,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>    \n",
    "<img src=\"./images/one-vs-rest.png\" alt=\"Drawing\" style=\"width: 1250px;\"/>\n",
    "**Figure 5**. Multiclass classification vs binary classification \n",
    "</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we include the M,F and I as 3 classes.\n",
    "# Here we turn into numpy arrays\n",
    "X_mul = df.iloc[:,1:].values\n",
    "y_mul = df.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features=['Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings','Years']\n",
    "\n",
    "n,m = 6,7\n",
    "mglearn.discrete_scatter(X_mul[:, n], X_mul[:, m], y_mul)\n",
    "plt.xlabel(features[n])\n",
    "plt.ylabel(features[m])\n",
    "plt.legend([\"Male: 0\", \"Female: 1\", \"Infant: 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X_mul[:,n:m+1], y_mul)\n",
    "print(\"Coefficient shape: \", linear_svm.coef_.shape)\n",
    "print(\"Intercept shape: \", linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X_mul[:, n], X_mul[:, m], y_mul)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(0, 30)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(features[n])\n",
    "plt.ylabel(features[m])\n",
    "plt.legend(['Male: 0', 'Female: 1', 'Infant: 2', 'Line class 0', 'Line class 1',\n",
    "'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about Linear Models\n",
    "\n",
    "* The main parameter of linear models is the regularization parameter (L1 or L2). If you assume that only a few of your features are actually important, you should use L1. Otherwise, you should use the default  L2. L1 can also be useful if interpretability of the model is important.\n",
    "\n",
    "* Linear models are very fast to train, and also fast to predict. They scale to very large datasets and work well with sparse data.\n",
    "\n",
    "* Linear models often perform well when the number of features is large compared to the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/questions.jpg\" style=\"width:1000px\">\n",
    "    ANY QUESTIONS?\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Fit three different linear models by testing different values of their respective parameters:\n",
    "    1. One model for predicting the amount of alcohol,\n",
    "    2. one for predicting the type of wine, and\n",
    "    3. one for predicting the quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/break.png\" style=\"width:1000px\">\n",
    "    15 min. Break\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive-Bayes Classifiers\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of independence between every pair of features.\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)} {P(x_1, \\dots, x_n)}\n",
    " $$\n",
    "\n",
    "Because the argument of independency\n",
    "\n",
    "$$ P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) \\\\\n",
    "\\Downarrow \\\\\n",
    "\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NB Family\n",
    "*  **Gaussian Naive Bayes**:\n",
    "$$ P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right) $$\n",
    "\n",
    "* **Multinomial Naive Bayes**:\n",
    "   The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$,\n",
    "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
    "   where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
    "   \n",
    "* **Bernoulli Naive Bayes:** Multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.\n",
    "$$P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/NB_scheme.png\" style=\"width:1800px\">\n",
    "We estimate $P(x$$_{\\alpha}$$|y)$ independently in each dimension (middle two images) and then obtain an estimate of the full data distribution by assuming conditional independence $P(x|y)=$$\\prod_{\\alpha}$$P(x$$_{\\alpha}$$|y)$ (very right image).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Multinomial NB is a NB with a multinomial distribution. We assume the data is distributed following a multinomial distribution. Eg. distribution of words in texts.\n",
    "* Bernoulli NB assumes a binary distribution of data. Eg. Text after using the bag-of-words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_mul,y_mul).predict(X_mul)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(X_mul.shape[0],(y_mul != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_mul, y_mul, random_state=42)\n",
    "gnb_fit = gnb.fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(gnb_fit.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(gnb_fit.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about NB Classifiers\n",
    "\n",
    "*  GaussianNB is mostly used on very high-dimensional data, while the other two variants of naive Bayes are widely used for sparse count data such as text. MultinomialNB usually performs better than BinaryNB , particularly on datasets with a relatively large number of nonzero features (i.e., large documents).\n",
    "\n",
    "* The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision.\n",
    "\n",
    "<center>    \n",
    "<img src=\"./images/02-DT.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "**Figure 6**. A decision tree to distinguish among several animals\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bin,y_bin, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"Male\", \"Female\"],\n",
    "feature_names=new_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Note: you should install graphviz in your system. \n",
    "#! pip install graphviz\n",
    "import graphviz\n",
    "with open('tree.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model):\n",
    "    n_features = X_bin.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), new_names)\n",
    "    plt.title('Feature importance for Abalon dataset M/F')\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Decision trees for regression\n",
    "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions about Decision Trees Classifier\n",
    "\n",
    "* One of the main drawbacks of decision trees is the tendency to overfit and provide poor generalization performance.\n",
    "\n",
    "* Usually, picking one of the pre-pruning strategies by setting either *max_depth* , *max_leaf_nodes* , or   *min_samples_leaf* is sufficient to prevent overfitting.\n",
    "\n",
    "* The resulting model can easily be visualized and understood by nonexperts (at least for smaller trees), and the algorithms are completely invariant to scaling of the data.\n",
    "\n",
    "* Decision trees do not have the ability to generate *new* responses, outside of what was seen in the training data. This shortcoming applies to all models based on trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles of Decision Trees\n",
    "\n",
    "Ensembles are methods that combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "* Random forests\n",
    "\n",
    "* Gradient boosted regression trees (gradient boosting machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mul,y_mul, random_state=42)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance for Random Forest\n",
    "plot_feature_importances(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Random Forest\n",
    "\n",
    "* Random forests for regression and classification are currently among the most widely used machine learning methods.\n",
    "\n",
    "* Easy parallelization.\n",
    "\n",
    "* Random forests don’t tend to perform well on very high dimensional, sparse data, such as text data. For this kind of data, linear models might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient boosted trees (gradient boosting machines)\n",
    "\n",
    "* The gradient boosted  tree is another ensemble method that combines multiple decision trees to create a more powerful model.\n",
    "\n",
    "* Use for both regression and classification. \n",
    "\n",
    "* Gradient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one.\n",
    "\n",
    "* Combine many simple models like shallow trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mul,y_mul, random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "plot_feature_importances(gbrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center>    \n",
    "<img src=\"./images/mistakes.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "How to be the Best of the Best (from Kaggle's point of view)\n",
    "</center>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "learns from the mistakes by increasing the weight of misclassified data points.\n",
    "\n",
    "* AdaBoost (Adaptive Boosting):  \n",
    "\n",
    ">Gradient boosting learns from the mistake — residual error directly, rather than update the weights of data points. \n",
    "\n",
    "* XGBoost \n",
    "* Catboost\n",
    "* LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Gradient boosted regression trees\n",
    "\n",
    "* They are among the most powerful and widely used models for supervised learning. \n",
    "\n",
    "* Their main drawback is that they require careful tuning of the parameters and may take a long time to train.\n",
    "\n",
    "* They are usually the winner methods in competitions such as Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernelized Support Vector Machines\n",
    "\n",
    "Kernelized support vector machines (SVMs) are an extension of Linear Support Vector Machines that allows for more complex models that are not defined simply by hyperplanes in the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "plt.rcParams['lines.markersize'] = 25.0\n",
    "\n",
    "\n",
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def plot_lin_svm():\n",
    "    linear_svm = LinearSVC().fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_lin_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "mask = y == 0\n",
    "\n",
    "def plot_3ln_svm():\n",
    "\n",
    "\n",
    "    figure = plt.figure()\n",
    "    # visualize in 3D\n",
    "    ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "    # plot first all the points with y == 0, then all with y == 1\n",
    "   \n",
    "    ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.set_xlabel(\"feature0\")\n",
    "    ax.set_ylabel(\"feature1\")\n",
    "    ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_3ln_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "\n",
    "\n",
    "def plot_3plane_svm():\n",
    "    \n",
    "\n",
    "\n",
    "    # show linear decision boundary\n",
    "    figure = plt.figure()\n",
    "    ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "    ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "    ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "    cmap=mglearn.cm2, s=60)\n",
    "    ax.set_xlabel(\"feature0\")\n",
    "    ax.set_ylabel(\"feature1\")\n",
    "    ax.set_zlabel(\"feature0 ** 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_3plane_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_3proj_svm():\n",
    "\n",
    "    ZZ = YY ** 2\n",
    "    dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "    plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "    cmap=mglearn.cm2, alpha=0.5)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_3proj_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# radial basis function (RBF) kernel, also known as the Gaussian kernel.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def plot_svc_rbf():\n",
    "    X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "    svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "    # plot support vectors\n",
    "    sv = svm.support_vectors_\n",
    "    # class labels of support vectors are given by the sign of the dual coefficients\n",
    "    sv_labels = svm.dual_coef_.ravel() > 0\n",
    "    mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_svc_rbf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mul_svc_rbf():\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    for ax, C in zip(axes, [-1, 0, 3]):\n",
    "        for a, gamma in zip(ax, range(-1, 2)):\n",
    "            mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "    axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "    ncol=4, loc=(.25, 1.2),fontsize='25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_mul_svc_rbf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mul,y_mul, random_state=42)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features=['Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings','Years']\n",
    "\n",
    "def plot_feats_svc_rbf():\n",
    "\n",
    "    plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n",
    "    plt.xticks(range(len(features)), names, rotation=90)\n",
    "    plt.plot(X_train.max(axis=0), '^', label=\"max\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.xlabel(\"Feature index\")\n",
    "    plt.ylabel(\"Feature magnitude\")\n",
    "\n",
    "# For SVN Preprocessing is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_feats_svc_rbf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions about Kernelized Support Vector Machines\n",
    "\n",
    "* SVMs allow for complex decision boundaries, even if the data has only a few features.\n",
    "\n",
    "* They work very well on high- and low- dimensional data.\n",
    "\n",
    "* Quite poor scaling with the number of samples (more than 100k can be a headache).\n",
    "\n",
    "* SVMs require very careful tuning of parameters and preprocesing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_questions.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_hands-on.jpg\" style=\"width:1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def foooo(h =.02): # step size in the mesh\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "             \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        MLPClassifier(alpha=1),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=1, n_clusters_per_class=1)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    datasets = [make_moons(noise=0.3, random_state=0),\n",
    "                make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "                linearly_separable\n",
    "                ]\n",
    "\n",
    "    figure = plt.figure(figsize=(30, 12))\n",
    "    i = 1\n",
    "    # iterate over datasets\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        # preprocess dataset, split into training and test part\n",
    "        X, y = ds\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        # just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(\"Input data\")\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "                   edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test)\n",
    "\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                       edgecolors='k')\n",
    "            # Plot the testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                       edgecolors='k', alpha=0.6)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            if ds_cnt == 0:\n",
    "                ax.set_title(name)\n",
    "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                    size=15, horizontalalignment='right')\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['lines.markersize'] = 12.0\n",
    "plt.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "foooo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion I\n",
    "\n",
    "* **Nearest neighbors**: For small datasets, good as a baseline, easy to explain.\n",
    "* **Linear models**: Go-to as a first algorithm to try, good for very large datasets, good for very highdimensional data.\n",
    "* **Naive Bayes**: Only for classification. Even faster than linear models, good for very large data sets and high-dimensional data. Often less accurate than linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion II\n",
    "\n",
    "* **Decision trees**: Very fast, don't need scaling of the data, can be visualized and easily explained. They don't predict on new data out of their training set.\n",
    "* **Random forests**: Nearly always perform better than a single decision tree, very robust and powerful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\n",
    "* **Gradient boosted decision trees**: Often slightly more accurate than random forests. Slower to train but faster to predict than random forests, and smaller in memory. Need more parameter tuning than random forests.\n",
    "* **Support vector machines**: Powerful for medium-sized datasets of features with similar meaning. Require scaling of data, sensitive to parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General Conclusion III\n",
    "\n",
    "* Copy and paste an algorithm for any given problem is the zero step. But first, check your data. The most important part is rehearse and explore different methods as much as you can on different interesting problems. \n",
    "\n",
    "* Simplicity is a virtue by itself. **Occam's razor**.\n",
    "\n",
    "* The best way to stand out over the crowd is try to understand the concepts (and the math) behind the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extras: Model selection in competitive data science vs real world\n",
    "\n",
    "* *Problem Definition*: The real world sucks! Use as much as possible domain knowledge.\n",
    "* *Metrics*: In the real world they can be very problem dependent. Remember the Goodhart's law.\n",
    "* *Interpretability*: People who pays for your time needs to know what you do.\n",
    "* *Data Quality*: Here is the line that divides the real of the kaggle world.\n",
    "* *Scalability*: Because benchmarking is nice, but real life is tough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bibliography:\n",
    "<center>\n",
    "<img src=\"./images/biblio.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "</center>\n",
    "\n",
    "* http://shop.oreilly.com/product/0636920030515.do\n",
    "* http://www-bcf.usc.edu/~gareth/ISL/\n",
    "* https://web.stanford.edu/~hastie/ElemStatLearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other sources:\n",
    "* https://coursera.org/learn/machine-learning\n",
    "* https://www.kdnuggets.com/\n",
    "\n",
    "## Recomended lecture:\n",
    "*A Few Useful Things to Know about Machine Learning* by Pedro Domingos (Communications of the ACM, Vol. 55 No. 10, Pages 78-87, 2012.):\n",
    "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"./images/00_thats_all.jpg\" style=\"width:1000px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_metadata": {
   "author": "Leonardo Espinosa",
   "title": "MLPP: Supervised Learning"
  },
  "livereveal": {
   "overlay": "<div class='myheader'><h2 class='headertekst'>Elementos de Machine Learning y Deep Learning. Universidad EAN </h2><h3 ><a href='#/21/1'>(index)</a></h3></div>",
   "progress": true,
   "scroll": true,
   "theme": "serif",
   "transition": "simple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
